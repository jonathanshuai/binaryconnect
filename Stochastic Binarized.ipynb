{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Connect Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable, Function\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 128\n",
    "n_epochs = 1000\n",
    "validation_steps = 10\n",
    "learning_rate = 5e-3\n",
    "stochastic = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=True, download=True,\n",
    "                        transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.5,), (0.5,))\n",
    "                       ])), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=False, \n",
    "                        transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.5,), (0.5,))\n",
    "                       ])), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binarize(Function):\n",
    "    @staticmethod        \n",
    "    def forward(ctx, input, weight, bias, stochastic=False):\n",
    "        if stochastic:\n",
    "            p_weight = torch.max(torch.zeros_like(weight), torch.min(torch.ones_like(weight), (weight + 1) / 2))\n",
    "            binarized_weights = torch.bernoulli(p_weight) * 2 - 1\n",
    "        else:\n",
    "            binarized_weights = torch.sign(weight)\n",
    "        \n",
    "        output = F.linear(input, binarized_weights, bias)\n",
    "        ctx.save_for_backward(input, binarized_weights, bias)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    @staticmethod    \n",
    "    def backward(ctx, gradients):\n",
    "        input, binarized_weight, bias = ctx.saved_tensors\n",
    "        grad_input = None\n",
    "        grad_weight = None\n",
    "        grad_bias = None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = gradients.mm(binarized_weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = gradients.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = gradients.sum(0)\n",
    "        \n",
    "#         print(gradients)\n",
    "#         return grad_input, torch.ones_like(grad_weight), grad_bias, None\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias, None\n",
    "\n",
    "class BinarizedLinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size, stochastic):\n",
    "        super(BinarizedLinear, self).__init__()\n",
    "        self.stochastic = stochastic\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight.data)\n",
    "#         torch.nn.init.xavier_uniform(self.fc.bias.data)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.fc.weight.data.clamp_(min=-1, max=1)\n",
    "        if self.stochastic and not self.training:\n",
    "            output = self.fc(input)\n",
    "        else:\n",
    "            output = Binarize.apply(input, self.fc.weight, self.fc.bias, self.stochastic)\n",
    "        return output\n",
    "\n",
    "class BinarizedDNNModel(nn.Module):\n",
    "    def __init__(self, image_size, output_size=10, hidden_size=1024, stochastic=False):\n",
    "        super(BinarizedDNNModel, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.stochastic = stochastic\n",
    "        self.fc1 = nn.Sequential(\n",
    "                   BinarizedLinear(image_size * image_size, hidden_size, stochastic),\n",
    "                   nn.ReLU(),\n",
    "                   nn.BatchNorm1d(hidden_size))\n",
    "        self.fc2 = nn.Sequential(\n",
    "                   BinarizedLinear(hidden_size, hidden_size, stochastic),\n",
    "                   nn.ReLU(),\n",
    "                   nn.BatchNorm1d(hidden_size)\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "                   BinarizedLinear(hidden_size, hidden_size, stochastic),\n",
    "                   nn.ReLU(),\n",
    "                   nn.BatchNorm1d(hidden_size))\n",
    "        self.output_layer = nn.Sequential(\n",
    "                    BinarizedLinear(hidden_size, output_size, stochastic),\n",
    "                    nn.ReLU(),\n",
    "                    nn.BatchNorm1d(output_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.image_size * self.image_size)\n",
    "        \n",
    "        for layer in [self.fc1, self.fc2, self.fc3, self.output_layer]:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class L2SVMLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(L2SVMLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, output, target):\n",
    "        y = one_hot_encoding(target)\n",
    "        ot = output * y\n",
    "        loss = torch.mean(torch.pow(F.relu(1 - ot), 2))\n",
    "        return loss\n",
    "    \n",
    "def one_hot_encoding(labels):\n",
    "    y = torch.eye(10) * 2 - 1\n",
    "    return y[labels].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dumdums(nn.Module):\n",
    "    def __init__(self, image_size, output_size=10, hidden_size=1024, stochastic=False):\n",
    "        super(dumdums, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.stochastic = stochastic\n",
    "        self.fc1 = nn.Sequential(\n",
    "                   BinarizedLinear(image_size * image_size, output_size, stochastic),\n",
    "                   nn.ReLU()\n",
    "#                    nn.BatchNorm1d(output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.image_size * self.image_size)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bl = BinarizedLinear(28 * 28, 32, stochastic=stochastic).to(device)\n",
    "# model = dumdums(image_size=28, stochastic=stochastic).to(device)\n",
    "# loss_function = L2SVMLoss().to(device)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = model(images.view(-1, 28 * 28))\n",
    "# o = torch.optim.SGD(model.parameters(), lr=1)\n",
    "# loss = outputs.mean()\n",
    "# loss.backward()\n",
    "# o.step()\n",
    "# model.fc1[0].fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fc1[0].fc.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinarizedDNNModel(image_size=28, stochastic=stochastic).to(device)\n",
    "loss_function = L2SVMLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0060, -0.0551, -0.0473,  ..., -0.0324, -0.0363,  0.0262],\n",
       "        [ 0.0284, -0.0271, -0.0083,  ..., -0.0391,  0.0121,  0.0484],\n",
       "        [ 0.0444, -0.0215, -0.0035,  ..., -0.0386, -0.0057, -0.0344],\n",
       "        ...,\n",
       "        [-0.0002, -0.0181, -0.0109,  ..., -0.0474,  0.0352, -0.0453],\n",
       "        [ 0.0076, -0.0486, -0.0331,  ...,  0.0208,  0.0525,  0.0497],\n",
       "        [-0.0533,  0.0404, -0.0516,  ..., -0.0511,  0.0401, -0.0077]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1[0].fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "========[EPOCH 0/1000]========\n",
      "[TRAIN ACCURACY]: 0.1476\n",
      "[TRAIN LOSS]: 282.7784\n",
      "[VALIDATION ACCURACY]: 0.1143\n",
      "========[EPOCH 1/1000]========\n",
      "[TRAIN ACCURACY]: 0.2232\n",
      "[TRAIN LOSS]: 159.1542\n",
      "========[EPOCH 2/1000]========\n",
      "[TRAIN ACCURACY]: 0.2740\n",
      "[TRAIN LOSS]: 146.0057\n",
      "========[EPOCH 3/1000]========\n"
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"========[EPOCH {}/{}]========\".format(epoch, n_epochs))\n",
    "    \n",
    "    # Training\n",
    "    train_acc = 0\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "#     print(model.fc1[0].fc.weight, model.fc1[0].fc.weight.grad)\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.data.cpu().numpy()\n",
    "        train_acc += np.mean(torch.argmax(outputs, 1).data.cpu().numpy() == labels.data.cpu().numpy())\n",
    "    \n",
    "#         if i % 100 == 0:\n",
    "#             print(model.fc1[0].fc.weight.grad.mean(0))\n",
    "    train_acc = train_acc / len(train_loader)\n",
    "    print(\"[TRAIN ACCURACY]: {:.4f}\".format(train_acc))\n",
    "    print(\"[TRAIN LOSS]: {:.4f}\".format(train_loss))\n",
    "\n",
    "    model.eval()\n",
    "    if epoch % validation_steps == 0:\n",
    "        # Validation\n",
    "        valid_acc = 0\n",
    "        for i, (images, labels) in enumerate(valid_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            valid_acc += np.mean(torch.argmax(outputs, 1).data.cpu().numpy() == labels.data.cpu().numpy())\n",
    "\n",
    "        valid_acc = valid_acc / len(valid_loader)\n",
    "        print(\"[VALIDATION ACCURACY]: {:.4f}\".format(valid_acc))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc1[0].fc.weight.grad.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for i in range(50000):\n",
    "    outputs = model(images)\n",
    "    loss = loss_function(outputs, labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(i, loss, np.mean(torch.argmax(outputs, 1).data.cpu().numpy() == labels.data.cpu().numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
